{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interfacing coffea background plotting with fileGrabber json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import uproot\n",
    "import awkward\n",
    "import graphviz\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils.histoHelpers as uhh\n",
    "import utils.uprootHelpers as uuh\n",
    "import mvatrain.preprocessors as mpp\n",
    "import matplotlib.pyplot as plt\n",
    "import coffea.processor as processor\n",
    "from mvatrain.metfilter import *\n",
    "from awkward import JaggedArray\n",
    "from os.path import join\n",
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "from coffea.processor import defaultdict_accumulator\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from mvatrain.ROCPlot import ROCPlot\n",
    "from mvatrain.hist_errorbars import hist_errorbars\n",
    "from FireHydrant.Tools.uproothelpers import NestNestObjArrayToJagged\n",
    "from FireHydrant.Tools.trigger import Triggers\n",
    "from FireHydrant.Tools.metfilter import MetFilters\n",
    "from FireHydrant.Tools.correction import get_pu_weights_function, get_ttbar_weight, get_nlo_weight_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore', over='ignore')\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "TIME_STR = \"190812\"\n",
    "TIME_STR2 = \"190815\"\n",
    "TIME_STR_CURRENT = \"190827\"\n",
    "DATA_DIR = join(os.environ[\"FFANA_BASE\"], f\"mvatrain/data\") #base data directory\n",
    "OUTPUT_DIR = join(os.environ[\"FFANA_BASE\"], f\"mvatrain/outputs/earl_grey_strong_redux\")  #model\n",
    "REPORT_DIR = join(os.environ[\"FFANA_BASE\"], f\"mvatrain/reports/{TIME_STR_CURRENT}\") #reports and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "model loaded.\n",
      "loading model...\n",
      "model loaded.\n",
      "loading model...\n",
      "model loaded.\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR_CURRENT = join(os.environ[\"FFANA_BASE\"], f\"mvatrain/outputs/earl_grey_strong_redux\")\n",
    "OUTPUT_DIR_MU = join(os.environ[\"FFANA_BASE\"], f\"mvatrain/outputs/english_breakfast_redux\")\n",
    "OUTPUT_DIR_ELEC = join(os.environ[\"FFANA_BASE\"], f\"mvatrain/outputs/irish_breakfast_redux\")\n",
    "\n",
    "print(\"loading model...\")\n",
    "xgbm_current = xgb.Booster({\"nthread\": 16})\n",
    "xgbm_current.load_model(join(OUTPUT_DIR_CURRENT, \"model_optimized/model.bin\"))\n",
    "if xgbm_current.attributes().get('SAVED_PARAM_predictor', None)=='gpu_predictor':\n",
    "    xgbm_current.set_attr(SAVED_PARAM_predictor=None)\n",
    "print(\"model loaded.\")\n",
    "\n",
    "print(\"loading model...\")\n",
    "xgbm_mu = xgb.Booster({\"nthread\": 16})\n",
    "xgbm_mu.load_model(join(OUTPUT_DIR_MU, \"model_optimized/model.bin\"))\n",
    "if xgbm_mu.attributes().get('SAVED_PARAM_predictor', None)=='gpu_predictor':\n",
    "    xgbm_mu.set_attr(SAVED_PARAM_predictor=None)\n",
    "print(\"model loaded.\")\n",
    "\n",
    "print(\"loading model...\")\n",
    "xgbm_elec = xgb.Booster({\"nthread\": 16})\n",
    "xgbm_elec.load_model(join(OUTPUT_DIR_ELEC, \"model_optimized/model.bin\"))\n",
    "if xgbm_elec.attributes().get('SAVED_PARAM_predictor', None)=='gpu_predictor':\n",
    "    xgbm_elec.set_attr(SAVED_PARAM_predictor=None)\n",
    "print(\"model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_opts = {\n",
    "    'edgecolor': (0,0,0,0.3),\n",
    "    'alpha': 0.8\n",
    "}\n",
    "error_opts = {\n",
    "    'label':'Stat. Unc.',\n",
    "    'hatch':'xxx',\n",
    "    'facecolor':'none',\n",
    "    'edgecolor':(0,0,0,.5),\n",
    "    'linewidth': 0\n",
    "}\n",
    "data_err_opts = {\n",
    "    'linestyle':'none',\n",
    "    'marker': '.',\n",
    "    'markersize': 10.,\n",
    "    'color':'k',\n",
    "    'elinewidth': 1,\n",
    "    'emarker': '_'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_ = json.load(open(\"2018.json\"))\n",
    "\n",
    "bkgdatasets = {}\n",
    "\n",
    "for group in datasets_.keys():\n",
    "    if \"DoubleMuon\" in group: continue\n",
    "    elif \"CRAB_PrivateMC\" in group: continue\n",
    "    else:\n",
    "        cutgroup = group[:-5]\n",
    "        if cutgroup == 'DYJetsToLL_M-50_TuneCP5_13TeV-madgraphMLM-pythia8_':\n",
    "            cutgroup = cutgroup[:-1]\n",
    "        if cutgroup not in bkgdatasets.keys(): bkgdatasets[cutgroup] = {'files': [], 'treename': \"\"}\n",
    "        bkgdatasets[cutgroup]['files'].extend(datasets_[group]['files'])\n",
    "        bkgdatasets[cutgroup]['treename'] = 'ffNtuplizer/ffNtuple'\n",
    "        \n",
    "counter = 0\n",
    "        \n",
    "bkgscales = {}\n",
    "for group in datasets_.keys():\n",
    "    if \"DoubleMuon\" in group: continue\n",
    "    elif \"CRAB_PrivateMC\" in group: continue\n",
    "    else:\n",
    "        if counter == 0:\n",
    "            cutgroup = group[:-5]\n",
    "            if cutgroup == 'DYJetsToLL_M-50_TuneCP5_13TeV-madgraphMLM-pythia8_':\n",
    "                cutgroup = cutgroup[:-1]\n",
    "            bkgscales[cutgroup] = datasets_[group]['weight']\n",
    "            previous = cutgroup\n",
    "        else:\n",
    "            if datasets_[group]['weight'] != bkgscales[previous]:\n",
    "                cutgroup = group[:-5]\n",
    "                if cutgroup == 'DYJetsToLL_M-50_TuneCP5_13TeV-madgraphMLM-pythia8_':\n",
    "                    cutgroup = cutgroup[:-1]\n",
    "                bkgscales[cutgroup] = datasets_[group]['weight']\n",
    "                previous = cutgroup\n",
    "                \n",
    "bkgmapping = {}\n",
    "for k in bkgdatasets: bkgmapping[k] = list(bkgdatasets[k])\n",
    "    \n",
    "datadatasets = {}\n",
    "for group in datasets_.keys():\n",
    "    if \"DoubleMuon\" not in group: continue\n",
    "    cutgroup = group[:-5]\n",
    "    if cutgroup == \"DoubleMuon_\":\n",
    "        cutgroup = cutgroup[:-1]\n",
    "    if \"Run2018A\" in cutgroup:\n",
    "        datadatasets['A']['files'] = []\n",
    "        datadatasets['A']['files'].extend(datasets_[group]['files'])\n",
    "        datadatasets['A']['treename'] = 'ffNtuplizer/ffNtuple'\n",
    "    elif \"Run2018B\" in cutgroup:\n",
    "        datadatasets['B']['files'] = []\n",
    "        datadatasets['B']['files'].extend(datasets_[group]['files'])\n",
    "        datadatasets['B']['treename'] = 'ffNtuples/ffNtuple'\n",
    "    elif \"Run2018C\" in cutgroup:\n",
    "        datadatasets['C']['files'] = []\n",
    "        datadatasets['C']['files'].extend(datasets_[group]['files'])\n",
    "        datadatasets['C']['treename'] = 'ffNtuples/ffNtuple'\n",
    "    elif \"Run2018D\" in cutgroup:\n",
    "        datadatasets['D']['files'] = []\n",
    "        datadatasets['D']['files'].extend(datasets_[group]['files'])\n",
    "        datadatasets['D']['treename'] = 'ffNtuples/ffNtuple'\n",
    "        \n",
    "datamapping = {'data': list('ABCD')}\n",
    "\n",
    "datasets = {}\n",
    "datasets.update(bkgdatasets)\n",
    "datasets.update(datadatasets)\n",
    "\n",
    "mapping = {}\n",
    "mapping.update(bkgmapping)\n",
    "mapping.update(datamapping)\n",
    "\n",
    "notdata = re.compile('(?!data)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeptonJetsProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        dataset_axis = hist.Cat('dataset', 'Control region')\n",
    "        pt_axis       = hist.Bin(\"pt\", \"pT [GeV]\", 50, 0, 800)\n",
    "        eta_axis      = hist.Bin(\"eta\", 'eta', 50, -2.4, 2.4)\n",
    "        neufrac_axis      = hist.Bin(\"neufrac\", \"neutral energy fraction\", 50, 0, 1.001)\n",
    "        maxd0_axis    = hist.Bin(\"maxd0\", 'track max |d0|', 50, 0, 0.5)\n",
    "        mind0_axis    = hist.Bin(\"mind0\", 'track min |d0|', 50, 0, 0.5)\n",
    "        tkiso_axis    = hist.Bin('tkiso', 'track isolation', 50, 0, 1)\n",
    "        pfiso_axis    = hist.Bin(\"pfiso\", \"PFCands isolation\", 50, 0, 1)\n",
    "        spreadpt_axis = hist.Bin(\"spreadpt\", \"spreadpt\", 50, 0, 1)\n",
    "        spreaddr_axis = hist.Bin(\"spreaddr\", \"spreaddr\", 50, 0, 0.1)\n",
    "        lambda_axis   = hist.Bin('lambda', 'jet sub - lambda', 50, -8, 0)\n",
    "        epsilon_axis  = hist.Bin('epsilon', 'jet sub - epsilon', 50, 0, 0.25)\n",
    "        ecf1_axis    = hist.Bin('ecf1', 'energy correlation function - e1', 50, 0, 200)\n",
    "        ecf2_axis    = hist.Bin('ecf2', 'energy correlation function - e2', 50, 0, 500)\n",
    "        ecf3_axis    = hist.Bin('ecf3', 'energy correlation function - e3', 50, 0, 300)\n",
    "        mva_axis      = hist.Bin('mva', 'BDT value', 50, -10, 10)\n",
    "        \n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'pt': hist.Hist(\"#counts/16GeV\", dataset_axis, pt_axis),\n",
    "            \"eta\": hist.Hist(\"#counts/0.096\", dataset_axis, eta_axis),\n",
    "            \"neufrac\": hist.Hist(\"#counts/0.02\", dataset_axis, neufrac_axis),\n",
    "            \"maxd0\": hist.Hist(\"#counts/0.01cm\", dataset_axis, maxd0_axis),\n",
    "            \"mind0\": hist.Hist(\"#counts/0.01cm\", dataset_axis, mind0_axis),\n",
    "            \"tkiso\": hist.Hist(\"#counts/0.02\", dataset_axis, tkiso_axis),\n",
    "            \"pfiso\": hist.Hist(\"#counts/0.02\", dataset_axis, pfiso_axis),\n",
    "            \"spreadpt\": hist.Hist(\"#counts/0.02\", dataset_axis, spreadpt_axis),\n",
    "            \"spreaddr\": hist.Hist(\"#counts/0.002\", dataset_axis, spreaddr_axis),\n",
    "            \"lambda\": hist.Hist(\"#counts/0.16\", dataset_axis, lambda_axis),\n",
    "            \"epsilon\": hist.Hist(\"#counts/0.005\", dataset_axis, epsilon_axis),\n",
    "            \"ecf1\": hist.Hist(\"#counts/4\", dataset_axis, ecf1_axis),\n",
    "            \"ecf2\": hist.Hist(\"#counts/10\", dataset_axis, ecf2_axis),\n",
    "            \"ecf3\": hist.Hist(\"#counts/6\", dataset_axis, ecf3_axis),\n",
    "            \"mva\": hist.Hist(\"#counts/0.4\", dataset_axis, mva_axis),\n",
    "        })\n",
    "        self.pucorrs = get_pu_weights_function()\n",
    "        self.nlo_w = get_nlo_weight_function('w')\n",
    "        self.nlo_z = get_nlo_weight_function('z')\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    def process(self, df):\n",
    "        output = self.accumulator.identity()\n",
    "        \n",
    "        dataset = df['dataset']\n",
    "        if dataset.startswith('TTJets_'): return output # skip TTJets_\n",
    "        \n",
    "        ## construct weights ##\n",
    "        wgts = processor.Weights(df.size)\n",
    "        \n",
    "        metfiltermask = np.logical_and.reduce([df[mf] for mf in MetFilters])\n",
    "        wgts.add('metfilters', metfiltermask)\n",
    "        \n",
    "        if len(dataset)!=1:\n",
    "            wgts.add('genw', df['weight'])\n",
    "\n",
    "            nvtx = df['trueInteractionNum']\n",
    "            pu, puUp, puDown = (f(nvtx) for f in self.pucorrs)\n",
    "            wgts.add('pileup', pu, puUp, puDown)\n",
    "\n",
    "            wnlo = np.ones_like(df.size)\n",
    "            if 'TTJets' in dataset or 'WJets' in dataset or 'DYJets' in dataset:\n",
    "                genparticles = JaggedCandidateArray.candidatesfromcounts(\n",
    "                    df['gen_p4'],\n",
    "                    px=df['gen_p4.fCoordinates.fX'],\n",
    "                    py=df['gen_p4.fCoordinates.fY'],\n",
    "                    pz=df['gen_p4.fCoordinates.fZ'],\n",
    "                    energy=df['gen_p4.fCoordinates.fT'],\n",
    "                    pid=df['gen_pid'],\n",
    "                )\n",
    "                gentops = genparticles[np.abs(genparticles.pid)==6]\n",
    "                genws = genparticles[np.abs(genparticles.pid)==24]\n",
    "                genzs = genparticles[np.abs(genparticles.pid)==23]\n",
    "\n",
    "                if 'TTJets' in dataset:\n",
    "                    wnlo = np.sqrt(get_ttbar_weight(gentops[0].p4.pt.sum()) * get_ttbar_weight(gentops[1].p4.pt.sum()))\n",
    "                elif 'WJets' in dataset:\n",
    "                    wnlo = self.nlo_w(genws[0].p4.pt.sum())\n",
    "                elif 'DYJets' in dataset:\n",
    "                    wnlo = self.nlo_z(genzs[0].p4.pt.sum())\n",
    "            wgts.add('nlo', wnlo)\n",
    "            \n",
    "        weight = wgts.weight()\n",
    "        ########################\n",
    "        \n",
    "        absd0 = np.abs(NestNestObjArrayToJagged(df['pfjet_pfcand_tkD0'])).fillna(0)\n",
    "        \n",
    "        leptonjets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['pfjet_p4'], **{\n",
    "            \"px\": df['pfjet_p4.fCoordinates.fX'],\n",
    "            \"py\": df['pfjet_p4.fCoordinates.fY'],\n",
    "            \"pz\": df['pfjet_p4.fCoordinates.fZ'],\n",
    "            \"energy\": df['pfjet_p4.fCoordinates.fT'],\n",
    "            \"neufrac\": ((df['pfjet_neutralEmE']+df['pfjet_neutralHadronE'])/df['pfjet_p4.fCoordinates.fT']),\n",
    "            \"maxd0\": absd0.max().content,\n",
    "            \"mind0\": absd0.min().content,\n",
    "            \"tkiso\": df['pfjet_tkIsolation05'],\n",
    "            \"pfiso\": df['pfjet_pfIsolation05'],\n",
    "            \"spreadpt\": df['pfjet_ptDistribution'],\n",
    "            \"spreaddr\": df['pfjet_dRSpread'],\n",
    "            \"lambda\": df['pfjet_subjet_lambda'],\n",
    "            \"epsilon\": df['pfjet_subjet_epsilon'],\n",
    "            \"ecf1\": df['pfjet_subjet_ecf1'],\n",
    "            \"ecf2\": df['pfjet_subjet_ecf2'],\n",
    "            \"ecf3\": df['pfjet_subjet_ecf3'],\n",
    "            })\n",
    "        \n",
    "        vals={\n",
    "            'target': leptonjets.pt.zeros_like().flatten(),\n",
    "            'pt': leptonjets.pt.flatten(),\n",
    "            'eta': leptonjets.eta.flatten(),\n",
    "        }\n",
    "        \n",
    "        vals.update( {k: leptonjets[k].flatten() for k in output.keys() if (k not in vals and k != 'mva')} )\n",
    "        \n",
    "        dfleptonjets = pd.DataFrame(vals)\n",
    "        \n",
    "        dfleptonjets.fillna(0)\n",
    "        \n",
    "        feature_cols = [n for n in dfleptonjets.keys() if n != \"target\"]\n",
    "\n",
    "        dfleptonjets2 = dfleptonjets[feature_cols]\n",
    "        \n",
    "        xglj = xgb.DMatrix(dfleptonjets2)\n",
    "        \n",
    "        predictions = xgbm_current.predict(xglj)\n",
    "        \n",
    "        offsets = leptonjets.pt.offsets\n",
    "        jaggedpredictions = JaggedArray.fromoffsets(offsets, predictions)\n",
    "        leptonjets.add_attributes(mva=jaggedpredictions)\n",
    "        \n",
    "        twoleptonjets = leptonjets.counts>=2\n",
    "        dileptonjets = leptonjets[twoleptonjets]\n",
    "        wgt = weight[twoleptonjets]\n",
    "        \n",
    "        leptonjetpair = dileptonjets.distincts()\n",
    "        sumpt = leptonjetpair.i0.pt+leptonjetpair.i1.pt\n",
    "        if sumpt.size==0: return output\n",
    "        \n",
    "        leadingLjPair = leptonjetpair[sumpt.argmax()]\n",
    "        controlregion = np.abs(leadingLjPair.i0.p4.delta_phi(leadingLjPair.i1.p4))<2.5\n",
    "        leptonjets_ = dileptonjets[controlregion.flatten()]\n",
    "        wgt = wgt[controlregion.flatten()]\n",
    "        wgts= (leptonjets_.pt.ones_like()*wgt).flatten()\n",
    "\n",
    "        output['pt'].fill(dataset=dataset, pt=leptonjets_.pt.flatten(), weight=wgts)\n",
    "        output['eta'].fill(dataset=dataset, eta=leptonjets_.eta.flatten(), weight=wgts)\n",
    "        output['neufrac'].fill(dataset=dataset, neufrac=leptonjets_.neufrac.flatten(), weight=wgts)\n",
    "        output['maxd0'].fill(dataset=dataset, maxd0=leptonjets_.maxd0.flatten(), weight=wgts)\n",
    "        output['mind0'].fill(dataset=dataset, mind0=leptonjets_.mind0.flatten(), weight=wgts)\n",
    "        output['tkiso'].fill(dataset=dataset, tkiso=leptonjets_.tkiso.flatten(), weight=wgts)\n",
    "        output['pfiso'].fill(dataset=dataset, pfiso=leptonjets_.pfiso.flatten(), weight=wgts)\n",
    "        output['spreadpt'].fill(dataset=dataset, spreadpt=leptonjets_.spreadpt.flatten(), weight=wgts)\n",
    "        output['spreaddr'].fill(dataset=dataset, spreaddr=leptonjets_.spreaddr.flatten(), weight=wgts)\n",
    "        output['lambda']  .fill(**{\"dataset\": dataset, \"lambda\": leptonjets_['lambda'].flatten(), 'weight': wgts})\n",
    "        output['epsilon'].fill(dataset=dataset, epsilon=leptonjets_.epsilon.flatten(), weight=wgts)\n",
    "        output['ecf1'].fill(dataset=dataset, ecf1=leptonjets_.ecf1.flatten(), weight=wgts)\n",
    "        output['ecf2'].fill(dataset=dataset, ecf2=leptonjets_.ecf2.flatten(), weight=wgts)\n",
    "        output['ecf3'].fill(dataset=dataset, ecf3=leptonjets_.ecf3.flatten(), weight=wgts)\n",
    "        output['mva'].fill(dataset=dataset, mva=leptonjets_.mva.flatten(), weight=wgts)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def postprocess(self, accumulator):\n",
    "        origidentity = list(accumulator)\n",
    "        \n",
    "        for k in origidentity:\n",
    "            # scale\n",
    "            accumulator[k].scale(bkgscales, axis='dataset')\n",
    "            # cat grouping\n",
    "            accumulator[k+'_cat'] = accumulator[k].group(hist.Cat(\"cat\", \"datasets\"), \"dataset\", mapping)\n",
    "        \n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 24/24 [00:11<00:00,  1.61it/s]\n",
      "Processing:   2%|▏         | 55/2306 [00:03<02:07, 17.72items/s]\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'KeyError'>: attribute lookup _KeyError on builtins failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/uscms/home/sadie/nobackup/anaconda3/envs/ffAna/lib/python3.7/concurrent/futures/process.py\", line 232, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/uscms/home/sadie/nobackup/anaconda3/envs/ffAna/lib/python3.7/site-packages/coffea/processor/executor.py\", line 86, in _work_function\n    tree = file[treename]\n  File \"/uscms/home/sadie/nobackup/anaconda3/envs/ffAna/lib/python3.7/site-packages/uproot/rootio.py\", line 221, in __getitem__\n    return self.get(name)\n  File \"/uscms/home/sadie/nobackup/anaconda3/envs/ffAna/lib/python3.7/site-packages/uproot/rootio.py\", line 319, in get\n    out = out.get(n, cycle)\n  File \"/uscms/home/sadie/nobackup/anaconda3/envs/ffAna/lib/python3.7/site-packages/uproot/rootio.py\", line 341, in get\n    raise _KeyError(\"not found: {0}\\n in file: {1}\".format(repr(name), self._context.sourcepath))\n_KeyError: not found: b'ffNtuplizer'\n in file: root://cmseos.fnal.gov//eos/uscms/store/group/lpcmetx/MCSIDM/ffNtuple/2018/WZ_TuneCP5_13TeV-pythia8/RunIIAutumn18DRPremix-102X_upgrade2018_realistic_v15-v3/190625_204127/0000/ffNtuple_18.root\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/uscms/home/sadie/nobackup/anaconda3/envs/ffAna/lib/python3.7/concurrent/futures/process.py\", line 198, in _sendback_result\n    exception=exception))\n  File \"/uscms/home/sadie/nobackup/anaconda3/envs/ffAna/lib/python3.7/multiprocessing/queues.py\", line 358, in put\n    obj = _ForkingPickler.dumps(obj)\n  File \"/uscms/home/sadie/nobackup/anaconda3/envs/ffAna/lib/python3.7/multiprocessing/reduction.py\", line 51, in dumps\n    cls(buf, protocol).dump(obj)\n_pickle.PicklingError: Can't pickle <class 'KeyError'>: attribute lookup _KeyError on builtins failed\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a1dee974a10e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                   \u001b[0mexecutor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures_executor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                   \u001b[0mexecutor_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                   \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                                  )\n",
      "\u001b[0;32m~/nobackup/anaconda3/envs/ffAna/lib/python3.7/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mrun_uproot_job\u001b[0;34m(fileset, treename, processor_instance, executor, executor_args, chunksize, maxchunks)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0mwrapped_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'metrics'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mexecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_work_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexecutor_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0mprocessor_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecutor_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'savemetrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/anaconda3/envs/ffAna/lib/python3.7/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mfutures_executor\u001b[0;34m(items, function, accumulator, workers, status, unit, desc, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mfutures_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/anaconda3/envs/ffAna/lib/python3.7/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mfutures_handler\u001b[0;34m(futures_set, output, status, unit, desc, futures_accumulator)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mfinished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures_set\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     \u001b[0mfutures_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                     \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/anaconda3/envs/ffAna/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/anaconda3/envs/ffAna/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'KeyError'>: attribute lookup _KeyError on builtins failed"
     ]
    }
   ],
   "source": [
    "output = processor.run_uproot_job(datasets,\n",
    "                                  treename=None,\n",
    "                                  processor_instance=LeptonJetsProcessor(),\n",
    "                                  executor=processor.futures_executor,\n",
    "                                  executor_args=dict(workers=12, flatten=True),\n",
    "                                  chunksize=500000,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

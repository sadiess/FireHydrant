{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interfacing Coffea processor/plotting with the xgboost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a clean version of the coffea processor notebook I've been working on in an attempt to interface the coffea processor and histogramming tools with the xgboost BDT model, so as to be able to plot BDT prediction values using the coffea histogramming and plotting tools. This way, alternate models could be substituted in, and predictions are made as part of the processor, rather than saved in the ntuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, imports (though not all of these are probably necessary for this notebook, they are the ones I have in my longer notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.seterr(divide='ignore', invalid='ignore', over='ignore')\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import uproot\n",
    "import awkward\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils.histoHelpers as uhh\n",
    "import utils.uprootHelpers as uuh\n",
    "import mvatrain.preprocessors as mpp\n",
    "import matplotlib.pyplot as plt\n",
    "import coffea.processor as processor\n",
    "from mvatrain.metfilter import *\n",
    "from os.path import join\n",
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "from coffea.processor import defaultdict_accumulator\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from mvatrain.ROCPlot import ROCPlot\n",
    "from mvatrain.hist_errorbars import hist_errorbars\n",
    "from FireHydrant.Tools.uproothelpers import NestNestObjArrayToJagged\n",
    "from FireHydrant.Tools.trigger import Triggers\n",
    "from FireHydrant.Tools.metfilter import MetFilters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the directory for our model (I have various models trained, earl_grey_strong was trained on a mix of background + 2mu2e signal + 4mu signal, for a longer training time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = join(os.environ[\"FFANA_BASE\"], f\"mvatrain/outputs/earl_grey_strong\")  #model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the model so that we can use it to make predictions later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "model loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"loading model...\")\n",
    "xgbm_default = xgb.Booster({\"nthread\": 16})\n",
    "xgbm_default.load_model(join(OUTPUT_DIR, \"model_default/model.bin\"))\n",
    "xgbm_optimized = xgb.Booster({\"nthread\": 16})\n",
    "xgbm_optimized.load_model(join(OUTPUT_DIR, \"model_optimized/model.bin\"))\n",
    "print(\"model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the files from the long 2018.json file, and split them into background and signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting background files from .json...\n",
      "Getting signal files from .json...\n",
      "Files gotten!\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting background files from .json...\")\n",
    "datasets_ = json.load(open('2018.json'))\n",
    "bkgdatasets = {}\n",
    "for group in datasets_.keys():\n",
    "    if 'DoubleMuon' in group: continue #not data\n",
    "    if 'CRAB_PrivateMC' in group: continue #not signal\n",
    "    files = datasets_[group]['files']\n",
    "    bkgdatasets[group] = [files] #normal background\n",
    "            \n",
    "print(\"Getting signal files from .json...\")\n",
    "datasets_ = json.load(open('2018.json'))\n",
    "sigdatasets = []\n",
    "for group in datasets_.keys():\n",
    "    if not ('CRAB_PrivateMC' in group): continue #not data or background\n",
    "    files = datasets_[group]['files']\n",
    "    for file in files: sigdatasets.append(file)\n",
    "    \n",
    "print(\"Files gotten!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the signal files into 2mu2e and 4mu subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lssig4mu = []\n",
    "lssig2mu2e = []\n",
    "\n",
    "for path in sigdatasets:\n",
    "    if '4Mu' in path:\n",
    "        lssig4mu.append(path)\n",
    "    else:\n",
    "        lssig2mu2e.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put those subsets into a dictionary with treenames so that the processor can read them later:/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dict(\n",
    "    sig4mu={'files': [], 'treename': 'ffNtuplizer/ffNtuple'},\n",
    "    sig2mu2e={'files': [], 'treename': 'ffNtuplizer/ffNtuple'},\n",
    ")\n",
    "\n",
    "dataset['sig4mu']['files'].extend(lssig4mu)\n",
    "dataset['sig2mu2e']['files'].extend(lssig2mu2e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set up our coffea processor to predictions and back-save predictions into an output so that we can plot them associated with our respective events (this is currently the part that's broken):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeptonJetProcessor(processor.ProcessorABC):\n",
    "    def __init__(self):\n",
    "        dataset_axis = hist.Cat('dataset', 'signal datasets')\n",
    "        match_axis   = hist.Cat('match', 'matched')\n",
    "        pt_axis       = hist.Bin(\"pt\", \"pT [GeV]\", 50, 0, 800)\n",
    "        eta_axis      = hist.Bin(\"eta\", 'eta', 50, -2.4, 2.4)\n",
    "        nef_axis      = hist.Bin(\"nef\", \"neutral energy fraction\", 50, 0, 1)\n",
    "        maxd0_axis    = hist.Bin(\"maxd0\", 'track max |d0|', 50, 0, 0.5)\n",
    "        mind0_axis    = hist.Bin(\"mind0\", 'track min |d0|', 50, 0, 0.5)\n",
    "        tkiso_axis    = hist.Bin('tkiso', 'track isolation', 50, 0, 1)\n",
    "        pfiso_axis    = hist.Bin(\"pfiso\", \"PFCands isolation\", 50, 0, 1)\n",
    "        spreadpt_axis = hist.Bin(\"spreadpt\", \"spreadpt\", 50, 0, 1)\n",
    "        spreaddr_axis = hist.Bin(\"spreaddr\", \"spreaddr\", 50, 0, 0.1)\n",
    "        lambda_axis   = hist.Bin('lamb', 'jet sub - lambda', 50, -8, 0)\n",
    "        epsilon_axis  = hist.Bin('epsi', 'jet sub - epsilon', 50, 0, 0.25)\n",
    "        ecfe1_axis    = hist.Bin('ecfe1', 'energy correlation function - e1', 50, 0, 750)\n",
    "        ecfe2_axis    = hist.Bin('ecfe2', 'energy correlation function - e2', 50, 0, 2000)\n",
    "        ecfe3_axis    = hist.Bin('ecfe3', 'energy correlation function - e3', 50, 0, 1000)\n",
    "        \n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'pt': hist.Hist(\"#counts/16GeV\", dataset_axis, pt_axis, match_axis),\n",
    "            \"eta\": hist.Hist(\"#counts/0.096\", dataset_axis, eta_axis, match_axis),\n",
    "            \"nef\": hist.Hist(\"#counts/0.02\", dataset_axis, nef_axis, match_axis),\n",
    "            \"maxd0\": hist.Hist(\"#counts/0.01cm\", dataset_axis, maxd0_axis, match_axis),\n",
    "            \"mind0\": hist.Hist(\"#counts/0.01cm\", dataset_axis, mind0_axis, match_axis),\n",
    "            \"tkiso\": hist.Hist(\"#counts/0.02\", dataset_axis, tkiso_axis, match_axis),\n",
    "            \"pfiso\": hist.Hist(\"#counts/0.02\", dataset_axis, pfiso_axis, match_axis),\n",
    "            \"spreadpt\": hist.Hist(\"#counts/0.02\", dataset_axis, spreadpt_axis, match_axis),\n",
    "            \"spreaddr\": hist.Hist(\"#counts/0.002\", dataset_axis, spreaddr_axis, match_axis),\n",
    "            \"lamb\": hist.Hist(\"#counts/0.16\", dataset_axis, lambda_axis, match_axis),\n",
    "            \"epsi\": hist.Hist(\"#counts/0.005\", dataset_axis, epsilon_axis, match_axis),\n",
    "            \"ecfe1\": hist.Hist(\"#counts/25\", dataset_axis, ecfe1_axis, match_axis),\n",
    "            \"ecfe2\": hist.Hist(\"#counts/40\", dataset_axis, ecfe2_axis, match_axis),\n",
    "            \"ecfe3\": hist.Hist(\"#counts/20\", dataset_axis, ecfe3_axis, match_axis),\n",
    "        })\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    def process(self, df):\n",
    "        output = self.accumulator.identity()\n",
    "        \n",
    "        dataset = df['dataset']  \n",
    "        \n",
    "        maxd0_ = np.abs(NestNestObjArrayToJagged(df['pfjet_pfcand_tkD0'])).fillna(0).max()\n",
    "        mind0_ = np.abs(NestNestObjArrayToJagged(df['pfjet_pfcand_tkD0'])).fillna(0).min()\n",
    "        \n",
    "        leptonjets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['pfjet_p4'],\n",
    "            px=df['pfjet_p4.fCoordinates.fX'],\n",
    "            py=df['pfjet_p4.fCoordinates.fY'],\n",
    "            pz=df['pfjet_p4.fCoordinates.fZ'],\n",
    "            energy=df['pfjet_p4.fCoordinates.fT'],\n",
    "            nef=(df['pfjet_neutralEmE']+df['pfjet_neutralHadronE'])/df['pfjet_p4.fCoordinates.fT'],\n",
    "            maxd0=maxd0_.content,\n",
    "            mind0=mind0_.content,\n",
    "            tkiso=df['pfjet_tkIsolation05'],\n",
    "            pfiso=df['pfjet_pfIsolation05'],\n",
    "            spreadpt=df['pfjet_ptDistribution'],\n",
    "            spreaddr=df['pfjet_dRSpread'],\n",
    "            lamb=df['pfjet_subjet_lambda'],\n",
    "            epsi=df['pfjet_subjet_epsilon'],\n",
    "            ecf1=df['pfjet_subjet_ecf1'],\n",
    "            ecf2=df['pfjet_subjet_ecf2'],\n",
    "            ecf3=df['pfjet_subjet_ecf3'],\n",
    "        )\n",
    "        \n",
    "        genparticles = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['gen_p4'],\n",
    "            px=df['gen_p4.fCoordinates.fX'],\n",
    "            py=df['gen_p4.fCoordinates.fY'],\n",
    "            pz=df['gen_p4.fCoordinates.fZ'],\n",
    "            energy=df['gen_p4.fCoordinates.fT'],\n",
    "            pid=df['gen_pid']\n",
    "        )\n",
    "        darkphotons = genparticles[genparticles.pid==32]\n",
    "        matchmask = leptonjets.match(darkphotons, deltaRCut=0.3)\n",
    "        \n",
    "        metfiltermask = np.logical_and.reduce([df[mf] for mf in MetFilters])\n",
    "        triggermask = np.logical_or.reduce([df[tp] for tp in Triggers])\n",
    "        \n",
    "        leptonjets_t = leptonjets[matchmask][metfiltermask&triggermask]\n",
    "        leptonjets_f = leptonjets[~matchmask][metfiltermask&triggermask]\n",
    "        \n",
    "        flatleptonjets_t = leptonjets_t.flatten()\n",
    "        flatleptonjets_f = leptonjets_f.flatten()\n",
    "        dfleptonjets_t = pd.DataFrame(flatleptonjets_t)\n",
    "        dfleptonjets_f = pd.DataFrame(flatleptonjets_f)\n",
    "        dfleptonjets_t.fillna(0)\n",
    "        dfleptonjets_f.fillna(0)\n",
    "        xglj_t = xgb.DMatrix(dfleptonjets_t)\n",
    "        xglj_f = xgb.DMatrix(dfleptonjets_f)\n",
    "        predictions_t = xgbm_optimized.predict(xglj_t)\n",
    "        predictions_f = xgbm_optimized.predict(xglj_f)\n",
    "        \n",
    "        offsets_t = leptonjets_t.offsets\n",
    "        offsets_f = leptonjets_f.offsets\n",
    "        jaggedpredictions_t = JaggedCandidateArray.candidatesfromoffsets(predictions_t, offsets_t)\n",
    "        jaggedpredictions_f = JaggedCandidateArray.candidatesfromoffsets(predictions_f, offsets_f)\n",
    "        \n",
    "        output['pt']      .fill(dataset=dataset, match='matched', pt=leptonjets_t.pt.flatten())\n",
    "        output['eta']     .fill(dataset=dataset, match='matched', eta=leptonjets_t.eta.flatten())\n",
    "        output['nef']     .fill(dataset=dataset, match='matched', nef=leptonjets_t.nef.flatten())\n",
    "        output['maxd0']   .fill(dataset=dataset, match='matched', maxd0=leptonjets_t.maxd0.flatten())\n",
    "        output['mind0']   .fill(dataset=dataset, match='matched', mind0=leptonjets_t.mind0.flatten())\n",
    "        output['tkiso']   .fill(dataset=dataset, match='matched', tkiso=leptonjets_t.tkiso.flatten())\n",
    "        output['pfiso']   .fill(dataset=dataset, match='matched', pfiso=leptonjets_t.pfiso.flatten())\n",
    "        output['spreadpt'].fill(dataset=dataset, match='matched', spreadpt=leptonjets_t.spreadpt.flatten())\n",
    "        output['spreaddr'].fill(dataset=dataset, match='matched', spreaddr=leptonjets_t.spreaddr.flatten())\n",
    "        output['lamb']    .fill(dataset=dataset, match='matched', lamb=leptonjets_t.lamb.flatten())\n",
    "        output['epsi']    .fill(dataset=dataset, match='matched', epsi=leptonjets_t.epsi.flatten())\n",
    "        output['ecfe1']   .fill(dataset=dataset, match='matched', ecfe1=leptonjets_t.ecf1.flatten())\n",
    "        output['ecfe2']   .fill(dataset=dataset, match='matched', ecfe2=leptonjets_t.ecf2.flatten())\n",
    "        output['ecfe3']   .fill(dataset=dataset, match='matched', ecfe3=leptonjets_t.ecf3.flatten())\n",
    "        output['mva']     .fill(dataset=dataset, match='matched', mva=jaggedpredictions_t.flatten())\n",
    "        \n",
    "        output['pt']      .fill(dataset=dataset, match='unmatched', pt=leptonjets_f.pt.flatten())\n",
    "        output['eta']     .fill(dataset=dataset, match='unmatched', eta=leptonjets_f.eta.flatten())\n",
    "        output['nef']     .fill(dataset=dataset, match='unmatched', nef=leptonjets_f.nef.flatten())\n",
    "        output['maxd0']   .fill(dataset=dataset, match='unmatched', maxd0=leptonjets_f.maxd0.flatten())\n",
    "        output['mind0']   .fill(dataset=dataset, match='unmatched', mind0=leptonjets_f.mind0.flatten())\n",
    "        output['tkiso']   .fill(dataset=dataset, match='unmatched', tkiso=leptonjets_f.tkiso.flatten())\n",
    "        output['pfiso']   .fill(dataset=dataset, match='unmatched', pfiso=leptonjets_f.pfiso.flatten())\n",
    "        output['spreadpt'].fill(dataset=dataset, match='unmatched', spreadpt=leptonjets_f.spreadpt.flatten())\n",
    "        output['spreaddr'].fill(dataset=dataset, match='unmatched', spreaddr=leptonjets_f.spreaddr.flatten())\n",
    "        output['lamb']    .fill(dataset=dataset, match='unmatched', lamb=leptonjets_f.lamb.flatten())\n",
    "        output['epsi']    .fill(dataset=dataset, match='unmatched', epsi=leptonjets_f.epsi.flatten())\n",
    "        output['ecfe1']   .fill(dataset=dataset, match='unmatched', ecfe1=leptonjets_f.ecf1.flatten())\n",
    "        output['ecfe2']   .fill(dataset=dataset, match='unmatched', ecfe2=leptonjets_f.ecf2.flatten())\n",
    "        output['ecfe3']   .fill(dataset=dataset, match='unmatched', ecfe3=leptonjets_f.ecf3.flatten())\n",
    "        output['mva']     .fill(dataset=dataset, match='unmatched', mva=jaggedpredictions_f.flatten())\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n",
      "Processing:   0%|          | 0/300 [00:00<?, ?items/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['target', 'pt', 'eta', 'neufrac', 'maxd0', 'mind0', 'tkiso', 'pfiso', 'spreadpt', 'spreaddr', 'lambda', 'epsilon', 'ecf1', 'ecf2', 'ecf3'] []\nexpected ecf1, ecf2, target, tkiso, spreadpt, mind0, spreaddr, epsilon, pt, ecf3, lambda, maxd0, neufrac, pfiso, eta in input data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/uscms/home/sadie/nobackup/anaconda3/envs/ffAna/lib/python3.7/concurrent/futures/process.py\", line 232, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/uscms/home/sadie/nobackup/anaconda3/envs/ffAna/lib/python3.7/site-packages/coffea/processor/executor.py\", line 90, in _work_function\n    out = processor_instance.process(df)\n  File \"<ipython-input-14-e5af4007cbc2>\", line 94, in process\n    predictions_t = xgbm_optimized.predict(xglj_t)\n  File \"/uscms/home/sadie/nobackup/anaconda3/envs/ffAna/lib/python3.7/site-packages/xgboost/core.py\", line 1284, in predict\n    self._validate_features(data)\n  File \"/uscms/home/sadie/nobackup/anaconda3/envs/ffAna/lib/python3.7/site-packages/xgboost/core.py\", line 1690, in _validate_features\n    data.feature_names))\nValueError: feature_names mismatch: ['target', 'pt', 'eta', 'neufrac', 'maxd0', 'mind0', 'tkiso', 'pfiso', 'spreadpt', 'spreaddr', 'lambda', 'epsilon', 'ecf1', 'ecf2', 'ecf3'] []\nexpected ecf1, ecf2, target, tkiso, spreadpt, mind0, spreaddr, epsilon, pt, ecf3, lambda, maxd0, neufrac, pfiso, eta in input data\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3255ce6f15ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                   \u001b[0mexecutor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures_executor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                   \u001b[0mexecutor_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                   \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                                  )\n",
      "\u001b[0;32m~/nobackup/anaconda3/envs/ffAna/lib/python3.7/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mrun_uproot_job\u001b[0;34m(fileset, treename, processor_instance, executor, executor_args, chunksize, maxchunks)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0mwrapped_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'metrics'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mexecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_work_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mexecutor_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0mprocessor_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecutor_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'savemetrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/anaconda3/envs/ffAna/lib/python3.7/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mfutures_executor\u001b[0;34m(items, function, accumulator, workers, status, unit, desc, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mfutures_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/anaconda3/envs/ffAna/lib/python3.7/site-packages/coffea/processor/executor.py\u001b[0m in \u001b[0;36mfutures_handler\u001b[0;34m(futures_set, output, status, unit, desc, futures_accumulator)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mfinished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures_set\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     \u001b[0mfutures_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                     \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/anaconda3/envs/ffAna/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/anaconda3/envs/ffAna/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['target', 'pt', 'eta', 'neufrac', 'maxd0', 'mind0', 'tkiso', 'pfiso', 'spreadpt', 'spreaddr', 'lambda', 'epsilon', 'ecf1', 'ecf2', 'ecf3'] []\nexpected ecf1, ecf2, target, tkiso, spreadpt, mind0, spreaddr, epsilon, pt, ecf3, lambda, maxd0, neufrac, pfiso, eta in input data"
     ]
    }
   ],
   "source": [
    "output = processor.run_uproot_job(dataset,\n",
    "                                  treename=None,\n",
    "                                  processor_instance=LeptonJetProcessor(),\n",
    "                                  executor=processor.futures_executor,\n",
    "                                  executor_args=dict(workers=12, flatten=True),\n",
    "                                  chunksize=500000,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
